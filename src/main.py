import os
import argparse
from dotenv import load_dotenv
from typing import List, Dict
from llm.extract import extract_ner, extract_keywords, extract_pos

# --- Disable Hugging Face Tokenizers Parallelism ---
# This avoids a warning message when tokenizers are used potentially before fork
os.environ["TOKENIZERS_PARALLELISM"] = "false"
# ---------------------------------------------------

from db.embeddings import get_embedding, store_chunks
from db.db import get_top_n_chunks
from reranker.rerank import rerank_chunks
from scraper.rtvslo_crawler import scrape_news, parse_response
from llm.openai_provider import OpenAIProvider
from llm.local_provider import LocalProvider
from llm.gemini_provider import GeminiProvider

# Load environment variables from .env file
load_dotenv()

# Select LLM Provider based on environment variable
openai_api_key = os.getenv("OPENAI_API_KEY")
google_api_key = os.getenv("GEMINI_API_KEY")
if google_api_key:
    print("--- Using Gemini Provider ---")

    llm_provider = GeminiProvider()
elif openai_api_key:
    print("--- Using OpenAI Provider ---")
    llm_provider = OpenAIProvider()
else:
    print("--- Using Local Provider (OpenAI API key not found) ---")
    llm_provider = LocalProvider()


def get_answer(
    query: str,
    history: List[Dict[str, str]],
    initial_k: int = 10,
    rerank_top_n: int = 3,
    scrape_k: int = 5,
) -> str:
    """
    Runs the full RAG pipeline: Scrape -> Store -> Embed Query -> Retrieve -> Rerank -> Generate.
    Fetches and stores new data for each query. Incorporates chat history.

    Args:
        query: The user's query string.
        history: The list of previous chat messages.
        initial_k: The number of chunks to retrieve initially from DB.
        rerank_top_n: The number of chunks to keep after reranking.
        scrape_k: The number of articles to attempt to scrape.

    Returns:
        The final answer generated by the LLM.
    """
    print(f"\nProcessing query: '{query}'")

    # 0. Scrape and Store New Data (Real-time component)
    print(f"--- Scraping and storing new data for query (top {scrape_k} results) ---")
    try:
        ner = extract_ner(query)
        print("NER Keywords: ", ner)
        keywords = extract_keywords(query)
        print("Keyword Keywords: ", keywords)
        pos = extract_pos(query)
        print("POS Keywords: ", pos)
        all_keywords = ner + keywords + pos
        print("All Keywords: ", all_keywords)
        news_response = scrape_news(all_keywords[0], per_page=scrape_k)
        articles = parse_response(news_response)
        print(f"Found {len(articles)} new articles.")
        if articles:
            for article in articles:
                print(f"Storing article: {article['title']}")
                # This function embeds and stores the chunks from the article
                store_chunks(article)
            print("Finished storing new articles.")
        else:
            print("No new articles found or parsed for this query.")
    except Exception as e:
        print(f"Error during scraping/storing: {e}. Proceeding with existing data.")
        # Decide if you want to return an error or just continue

    # 1. Embed the query
    print("--- Embedding query ---")
    try:
        query_embedding = get_embedding(query)
    except Exception as e:
        print(f"Error getting embedding: {e}")
        return "Sorry, I encountered an error processing your query embedding."
    # print(f"Query embedding type: {type(query_embedding)}") # Optional debug

    # 2. Initial Retrieval (from potentially updated DB)
    print("--- Retrieving initial chunks from DB ---")
    try:
        initial_chunks = get_top_n_chunks(query_embedding, n=initial_k)
    except Exception as e:
        print(f"Error retrieving chunks from DB: {e}")
        return "Sorry, I encountered an error retrieving information from the database."

    if not initial_chunks:
        print(
            "No relevant chunks found in the database (including any newly added ones)."
        )
        return "I couldn't find any relevant information to answer your question, even after searching online."

    print(f"Retrieved {len(initial_chunks)} initial chunks from DB.")
    # Optional: Print initial chunks for comparison
    # print("Top initial chunks (before reranking):")
    # for i, chunk in enumerate(initial_chunks[:3]): # Print top 3
    #     print(f"  {i+1}. Score: {chunk[2]:.4f} - {chunk[1][:80]}...")

    # 3. Reranking Step
    print("--- Reranking chunks ---")
    try:
        reranked_chunks = rerank_chunks(query, initial_chunks, top_n=rerank_top_n)
        # Optional: Print reranked chunks
        # print("Top reranked chunks:")
        # for i, chunk in enumerate(reranked_chunks):
        #     print(f"  {i+1}. Rerank Score: {chunk[2]:.4f} - {chunk[1][:80]}...")
    except Exception as e:
        print(f"Error during reranking: {e}. Skipping reranking.")
        # Fallback: use top N initial chunks if reranker fails
        reranked_chunks = initial_chunks[:rerank_top_n]

    # 4. Generation Step
    print("--- Generating final answer ---")
    if not reranked_chunks:
        # Handle case where even after fallback, there are no chunks
        # This might happen if initial_k < rerank_top_n and reranker fails
        print("No chunks available for generation after reranking/fallback.")
        return "I couldn't find enough relevant information to formulate an answer."

    try:
        # Combine chunk texts into a single context string
        context = "\n\n".join([chunk[1] for chunk in reranked_chunks])

        # Use the selected LLM provider instance to generate the answer
        # Pass the history along with the query and context
        final_answer = llm_provider.generate_answer(query, context, history)
    except Exception as e:
        print(f"Error during answer generation: {e}")
        return "Sorry, I encountered an error while formulating the final answer."

    return final_answer, all_keywords[0]


if __name__ == "__main__":
    # --- Argument Parsing ---
    parser = argparse.ArgumentParser(description="Run the RAG chatbot.")
    parser.add_argument(
        "--use-chat-history",
        action="store_true",
        help="Enable the use of chat history in prompts.",
    )
    args = parser.parse_args()
    # ------------------------

    print("\n--- Real-time Chatbot Ready ---")
    print("Each query will trigger web scraping and data storage before answering.")
    if args.use_chat_history:
        print("--- Chat history is ENABLED ---")
        chat_history: List[Dict[str, str]] = []
    else:
        print("--- Chat history is DISABLED ---")
        # No history will be stored or passed if the flag is not set

    while True:
        try:
            user_query = input("> ").strip()
            if not user_query:
                continue
            if user_query.lower() in ["quit", "exit"]:
                print("Exiting chatbot. Goodbye!")
                break

            # Determine history to pass based on the flag
            history_to_pass = chat_history if args.use_chat_history else []

            # Execute the pipeline, passing the appropriate history
            answer, _ = get_answer(user_query, history_to_pass, scrape_k=5)

            # Print the final answer
            print("\n--- ANSWER ---")
            print(answer)
            print("--------------")

            # Append current exchange to history only if enabled
            if args.use_chat_history:
                chat_history.append({"role": "user", "content": user_query})
                chat_history.append({"role": "assistant", "content": answer})

                # Optional: Limit history size
                # max_history_len = 10
                # if len(chat_history) > max_history_len:
                #     chat_history = chat_history[-max_history_len:]

        except EOFError:  # Handle Ctrl+D
            print("\nExiting chatbot. Goodbye!")
            break
        except KeyboardInterrupt:  # Handle Ctrl+C
            print("\nExiting chatbot. Goodbye!")
            break
        except Exception as e:
            print(f"An unexpected error occurred in the main loop: {e}")
            # Optionally add more robust error handling or logging here
