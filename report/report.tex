%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% FRI Data Science_report LaTeX Template
% Version 1.0 (28/1/2020)
% 
% Jure Demšar (jure.demsar@fri.uni-lj.si)
%
% Based on MicromouseSymp article template by:
% Mathias Legrand (legrand.mathias@gmail.com) 
% With extensive modifications by:
% Antonio Valente (antonio.luis.valente@gmail.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------
\documentclass[fleqn,moreauthors,10pt]{ds_report}
\usepackage[english]{babel}

\graphicspath{{fig/}}




%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

% Header
\JournalInfo{FRI Natural language processing course 2025}

% Interim or final report
\Archive{Project report} 
%\Archive{Final report} 

% Article title
\PaperTitle{Conversational Agent with Retrieval-Augmented Generation} 

% Authors (student competitors) and their info
\Authors{Blaž Špacapan, Matevž Jecl, Tilen Ožbot}

% Advisors
\affiliation{\textit{Advisors: Aleš Žagar}}

% Keywords
\Keywords{RAG, web-scraping, sports news}
\newcommand{\keywordname}{Keywords}


%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\Abstract{
This project, developed for the NLP course, presents a Python-based chatbot that maintains up-to-date sports news knowledge using a real-time Retrieval-Augmented Generation (RAG) pipeline. We implement automated web-scraping of rtvslo.si to fetch full articles and metadata, perform HTML parsing, text normalization and duplicate removal. Upon a user query, the system extracts keywords via a spaCy+LLM hybrid approach, scrapes and indexes new articles on the fly, retrieves and re-ranks the top semantically similar chunks, and synthesizes a coherent answer using a large language model—all within sub-second latency. We evaluate two RAG configurations, GAMS and Gemini, on our sports QA test suite, obtaining up to 57.22 BLEU and 0.753 ROUGE-L for the best model, and complement these with human judgments of coherence, informativeness, and factual consistency. A pilot study with classmates confirms the chatbot’s ability to deliver accurate, contextually relevant responses in dynamic sports scenarios, demonstrating that a lightweight real-time RAG system can be built effectively at the undergraduate level and extended to other news domains.
}

%----------------------------------------------------------------------------------------

\begin{document}

% Makes all text pages the same height
\flushbottom 

% Print the title and abstract box
\maketitle 

% Removes page numbering from the first page
\thispagestyle{empty} 

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section*{Introduction}
    Various fields such as customer service, education, and even general knowledge have shifted greatly with the introduction of sophisticated language models as conversational agents. Even with all the advancements made in the industry, conventional chatbots depend purely on pre-trained static knowledge, which can quickly become outdated, making it impossible to retrieve accurate information. The goal of this project is to address that gap by creating a chatbot that automatically improves its accuracy and responsiveness by web-scraping related news articles from the web, in real-time.
    The proposed conversational agent utilizes Retrieval-Augmented Generation (RAG) techniques to dynamically access current information, ensuring the responses provided are accurate and up to date.

    Master's thesis from Chalmers University of Technology exploring enhancements to conversational agents with Retrieval-Augmented Generation by integrating an autoregressive Large Language Model that generates real-time dense retrieval vectors, significantly improving multi-hop reasoning through synthetic data generation and attention-based relabeling to reduce hallucinations \cite{RAG1}.

    Research from ESP Journal of Engineering \& Technology Advancements introduces a conversational Retrieval-Augmented Generation framework designed for real-time crisis management, integrating structured and unstructured data sources to deliver validated, contextually relevant insights that enhance emergency decision-making and response effectiveness \cite{jeta2024}.

    In this article, the authors propose Retrieval-Augmented Generation (RAG) models that integrate parametric seq2seq generation with non-parametric retrieval mechanisms, demonstrating improved performance in conversational agents through enhanced factual accuracy and response specificity in knowledge-intensive interactions \cite{NEURIPS2020_6b493230}.

    The paper proposes a misinformation detection framework, FCRV (Full-Context Retrieval and Verification), that integrates claim extraction via Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) to construct a comprehensive context for news verification, significantly improving detection accuracy, robustness, and scalability against both human and AI-generated fake news \cite{10826000}.

    The paper demonstrates how Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs) effectively addresses challenges in extracting precise business event information from diverse and evolving data sources, significantly enhancing adaptability and accuracy in dynamic business environments \cite{ARSLAN20244534}.


%------------------------------------------------

\section*{Methods}

% Use the Methods section to describe what you did an how you did it -- in what way did you prepare the data, what algorithms did you use, how did you test various solutions ... Provide all the required details for a reproduction of your work.

% Below are \LaTeX examples of some common elements that you will probably need when writing your report (e.g. figures, equations, lists, code examples ...).

\subsection*{Data}

News content will be collected exclusively by web‑scraping major outlets. Each script retrieves full article text and metadata (title, author, publication date, URL) and stores the raw content for processing. A preprocessing pipeline then performs HTML parsing, text normalization, duplicate removal, and formatting. Cleaned articles are converted into vector representations and indexed in a vector database to enable efficient semantic retrieval. This approach produces a continuously updated, semantically organized corpus of both current and historical news articles, supporting rapid access to relevant documents for the conversational agent.

\subsection*{Database}

The system relies on a PostgreSQL database enhanced with the \texttt{pgvector} extension. While earlier versions used this database both to compute and store vector embeddings for each text fragment, embedding generation has since been moved upstream. Today, the database’s sole responsibility is to retrieve content of already fetched articles. The database schema consists of a single table, \texttt{content\_chunks}, which holds:
\begin{itemize}
  \item \texttt{id} (integer): a primary key for each chunk, generated by a sequence.
  \item \texttt{chunk\_text} (text): the raw text of the content fragment.
  \item \texttt{embedding} (vector(384)): the precomputed 384-dimensional embedding for similarity search.
  \item \texttt{source\_identifier} (text): a unique identifier such as a URL or filename for the original source.
  \item \texttt{created\_at} (\texttt{timestamptz}): a timestamp set at insertion.
  \item \texttt{metadata} (JSONB): arbitrary metadata (e.g., page numbers, tags) associated with the chunk.
\end{itemize}

\subsection*{Pipeline}

We selected \texttt{rtvslo.si} for initial testing because it offers robust filtering by category and date. For this phase, we focus exclusively on sports news. We use the following parameters when querying the \texttt{rtvslo.si} API:

\begin{itemize}
  \item \texttt{q}: the search query string
  \item \texttt{s}: section filter (e.g., \texttt{3} for sports, \texttt{null} for all)
  \item \texttt{sort}: sort order (\texttt{1} = newest first, \texttt{2} = most popular)
  \item \texttt{a}: time range (\texttt{1} = all time, \texttt{2} = last 24 h, \texttt{3} = last week, \texttt{4} = last month, \texttt{5} = last year)
  \item \texttt{per\_page}: number of results to return per page
  \item \texttt{group}: content type group (\texttt{1} = news, \texttt{15} = video, \texttt{16} = audio)
\end{itemize}

\begin{enumerate}
  \item \textbf{Receive user query}:  
    The system accepts a natural‑language question or request from the user.
  \item \textbf{Extract keywords from the query}:  
   Initially we use a spaCy-based named entity recognition (NER) method to identify key entities within the question. If no entities are found, the function assesses the ambiguity of the input using an LLM (\texttt{gemma-2-2b-it} or \texttt{gemma-3-4b-it}). If the question is deemed clear enough, keywords are extracted using an LLM-based approach that captures more nuanced information. However, if the text is found to be overly ambiguous, the function generates an explanatory output with a suggestion for a less ambiguous question and terminates further processing. This layered strategy enables efficient handling of clearly defined questions while providing robust feedback for ambiguous queries.
  \textbf{Example:}

   > Koliko točk je dobil na zadnji tekmi?

   \# To vprašanje je slabo zastavljeno, saj ne navaja konteksta (katero tekmo). \#
  
  \item \textbf{Perform web‑scraping to discover relevant articles}:  
    Using the extracted keywords, the scraper queries selected news pages to retrieve potentially relevant articles. Since \texttt{robots.txt} is not provided by the site, we enforce a 1s timeout between requests to avoid overwhelming the server. For enough context 50 pages were fetched.
  
  \item \textbf{Parse and collect article metadata and content}:  
    Relevant content from newly found articles is processed, converted into vector representations (embeddings), and stored in a database. The system retrieves a predefined number of the most semantically similar information chunks from the entire database (including newly added ones) based on the user's query embedding.

    \item \textbf{Chunk Reranking}: We re-rank all candidate chunks with the cross-encoder \textit{cross-encoder/ms-marco-MiniLM-L-6-v2}; chunks whose similarity score is $\ge 1.0$ are discarded.

    \item \textbf{LLM-based Answer Synthesis}: Finally, a large language model (\textit{GaMS-9B-Instruct-Q4\_k} or \textit{gemini-1.5-flash-latest}) receives the refined, relevant information chunks and the original user query (potentially along with conversation history). It synthesizes this information to generate a comprehensive and contextually appropriate answer.


\end{enumerate}

%----------------------------------------------------------------------------------------
%	RESULTS AND DISCUSSION
%----------------------------------------------------------------------------------------

\subsection*{Results}

We evaluate both RAG configurations on our sports QA test suite using automatic metrics (BLEU, ROUGE) and human judgments (coherence, informativeness, factual consistency) across 11 sampled QA pairs.

\paragraph{Automatic Metrics}
Table~\ref{tab:auto-metrics} shows BLEU and ROUGE scores, computed on answers generated \emph{with the identical retrieval + reranking pipeline} described above

\begin{table}[h]
  \centering
  \small
  \caption{Automatic evaluation results}
  \label{tab:auto-metrics}
  \begin{tabular}{lcccc}
    \toprule
    Model   & BLEU (\%) & ROUGE-1 & ROUGE-2 & ROUGE-L \\
    \midrule
    GAMS    & 30.37     & 0.455   & 0.307   & 0.418   \\
    Gemini  & 57.22     & 0.779   & 0.652   & 0.753   \\
    \bottomrule
  \end{tabular}
\end{table}

\paragraph{Human Evaluation}
\begin{itemize}
  \item \textbf{Coherence}: logical flow and structure
  \item \textbf{Informativeness}: amount of relevant information
  \item \textbf{Factual consistency}: alignment with ground truth
\end{itemize}
Overall, Gemini outperformed GAMS on every dimension. Table~\ref{tab:human-summary} summarizes average ratings.

\begin{table}[h]
  \centering
  \small
  \caption{Average human evaluation scores (1–5)}
  \label{tab:human-summary}
  \begin{tabular}{lccc}
    \toprule
    Model   & Coherence & Informativeness & Factual Consistency \\
    \midrule
    GAMS    & 4.2       & 4.0             & 3.1                 \\
    Gemini  & 4.8       & 4.5             & 4.7                 \\
    \bottomrule
  \end{tabular}
\end{table}

\paragraph{Error Analysis}
\begin{itemize}
  \item \textbf{Wrong answers by GAMS (despite context):}
    \begin{itemize}
      \item Q10 (\emph{Benjamin Šeško}): predicted “Eintracht Frankfurt” (correct: Stuttgart); factual score 1/5.
      \item Q8 (L. Dončić’s penultimate game): predicted 28 points (correct: 38); factual score 1/5.
      \item Q5 (J. Oblak’s last Atlético match): predicted 8 saves (correct: 6); factual score 1/5.
    \end{itemize}
  \item \textbf{Absence detection:}
    \begin{itemize}
      \item Q3 (winner of final Tour de France stage): both models correctly abstained (“not in context”); factual score 5/5.
    \end{itemize}
  \item \textbf{Consistently correct responses:}
    \begin{itemize}
      \item Q1 (L. Dončić’s last game): 28 points.
      \item Q11 (A. Kopitar’s last opponent): Edmonton.
    \end{itemize}
\end{itemize}

\subsection*{Discussion}

Our results demonstrate a clear advantage of the Gemini configuration over GAMS. The automatic metrics reveal a BLEU gain of 26.85 points and a ROUGE-L improvement of 0.335, indicating much stronger alignment with reference answers. Human evaluations corroborate these findings: Gemini achieves higher coherence (+0.6), informativeness (+0.5), and factual consistency (+1.6) on average.

GAMS’s recurrent factual errors, even when relevant context was retrieved, suggest limitations in its reranking or answer synthesis stages, leading to hallucinations. Although Gemini’s improved answer generation strategies substantially reduce misinformation compared to GAMS, it too occasionally generates minor inaccuracies even with the correct context, indicating that enhanced grounding and verification remain necessary.

Overall, these results validate that a lightweight, real-time RAG pipeline can be implemented effectively.  

%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------
\bibliographystyle{unsrt}
\bibliography{report}


\end{document}