%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% FRI Data Science_report LaTeX Template
% Version 1.0 (28/1/2020)
% 
% Jure Demšar (jure.demsar@fri.uni-lj.si)
%
% Based on MicromouseSymp article template by:
% Mathias Legrand (legrand.mathias@gmail.com) 
% With extensive modifications by:
% Antonio Valente (antonio.luis.valente@gmail.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------
\documentclass[fleqn,moreauthors,10pt]{ds_report}
\usepackage[english]{babel}

\graphicspath{{fig/}}




%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

% Header
\JournalInfo{FRI Natural language processing course 2025}

% Interim or final report
\Archive{Project report} 
%\Archive{Final report} 

% Article title
\PaperTitle{Conversational Agent with Retrieval-Augmented Generation} 

% Authors (student competitors) and their info
\Authors{Blaž Špacapan, Matevž Jecl, Tilen Ožbot}

% Advisors
\affiliation{\textit{Advisors: Aleš Žagar}}

% Keywords
\Keywords{RAG, web-scraping}
\newcommand{\keywordname}{Keywords}


%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\Abstract{
This project, developed for the NLP course, presents a Python-based chatbot that maintains up-to-date sports news knowledge using a real-time Retrieval-Augmented Generation (RAG) pipeline. We implement automated web-scraping of rtvslo.si to fetch full articles and metadata, perform HTML parsing, text normalization and duplicate removal. Upon a user query, the system extracts keywords via a spaCy+LLM hybrid approach, scrapes and indexes new articles on the fly, retrieves and re-ranks the top semantically similar chunks, and synthesizes a coherent answer using a large language model—all within sub-second latency. We evaluate two RAG configurations, GAMS and Gemini, on our sports QA test suite, obtaining up to 57.22 BLEU and 0.753 ROUGE-L for the best model, and complement these with human judgments of coherence, informativeness, and factual consistency. A pilot study with classmates confirms the chatbot’s ability to deliver accurate, contextually relevant responses in dynamic sports scenarios, demonstrating that a lightweight real-time RAG system can be built effectively at the undergraduate level and extended to other news domains.
}

%----------------------------------------------------------------------------------------

\begin{document}

% Makes all text pages the same height
\flushbottom 

% Print the title and abstract box
\maketitle 

% Removes page numbering from the first page
\thispagestyle{empty} 

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section*{Introduction}
    Various fields such as customer service, education, and even general knowledge have shifted greatly with the introduction of sophisticated language models as conversational agents. Even with all the advancements made in the industry, conventional chatbots depend purely on pre-trained static knowledge, which can quickly become outdated, making it impossible to retrieve accurate information. The goal of this project is to address that gap by creating a chatbot that automatically improves its accuracy and responsiveness by web-scraping related news articles from the web, in real-time.
    The proposed conversational agent utilizes Retrieval-Augmented Generation (RAG) techniques to dynamically access current information, ensuring the responses provided are accurate and up to date.

    Master's thesis from Chalmers University of Technology exploring enhancements to conversational agents with Retrieval-Augmented Generation by integrating an autoregressive Large Language Model that generates real-time dense retrieval vectors, significantly improving multi-hop reasoning through synthetic data generation and attention-based relabeling to reduce hallucinations \cite{RAG1}.

    Research from ESP Journal of Engineering \& Technology Advancements introduces a conversational Retrieval-Augmented Generation framework designed for real-time crisis management, integrating structured and unstructured data sources to deliver validated, contextually relevant insights that enhance emergency decision-making and response effectiveness \cite{jeta2024}.

    In this article, the authors propose Retrieval-Augmented Generation (RAG) models that integrate parametric seq2seq generation with non-parametric retrieval mechanisms, demonstrating improved performance in conversational agents through enhanced factual accuracy and response specificity in knowledge-intensive interactions \cite{NEURIPS2020_6b493230}.

    The paper proposes a misinformation detection framework, FCRV (Full-Context Retrieval and Verification), that integrates claim extraction via Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) to construct a comprehensive context for news verification, significantly improving detection accuracy, robustness, and scalability against both human and AI-generated fake news \cite{10826000}.

    The paper demonstrates how Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs) effectively addresses challenges in extracting precise business event information from diverse and evolving data sources, significantly enhancing adaptability and accuracy in dynamic business environments \cite{ARSLAN20244534}.


%------------------------------------------------

\section*{Methods}

% Use the Methods section to describe what you did an how you did it -- in what way did you prepare the data, what algorithms did you use, how did you test various solutions ... Provide all the required details for a reproduction of your work.

% Below are \LaTeX examples of some common elements that you will probably need when writing your report (e.g. figures, equations, lists, code examples ...).

\subsection*{Data}

News content will be collected exclusively by web‑scraping major outlets. Each script retrieves full article text and metadata (title, author, publication date, URL) and stores the raw content for processing. A preprocessing pipeline then performs HTML parsing, text normalization, duplicate removal, and formatting. Cleaned articles are converted into vector representations and indexed in a vector database to enable efficient semantic retrieval. This approach produces a continuously updated, semantically organized corpus of both current and historical news articles, supporting rapid access to relevant documents for the conversational agent.

\subsection{Database}

The system relies on a PostgreSQL database enhanced with the \texttt{pgvector} extension. While earlier versions used this database both to compute and store vector embeddings for each text fragment, embedding generation has since been moved upstream. Today, the database’s sole responsibility is to retrieve content of already fetched articles. The database schema consists of a single table, \texttt{content\_chunks}, which holds:
\begin{itemize}
  \item \texttt{id} (integer): a primary key for each chunk, generated by a sequence.
  \item \texttt{chunk\_text} (text): the raw text of the content fragment.
  \item \texttt{embedding} (vector(384)): the precomputed 384-dimensional embedding for similarity search.
  \item \texttt{source\_identifier} (text): a unique identifier such as a URL or filename for the original source.
  \item \texttt{created\_at} (\texttt{timestamptz}): a timestamp set at insertion.
  \item \texttt{metadata} (JSONB): arbitrary metadata (e.g., page numbers, tags) associated with the chunk.
\end{itemize}

\subsection*{Pipeline}

We selected \texttt{rtvslo.si} for initial testing because it offers robust filtering by category and date. For this phase, we focus exclusively on sports news. We use the following parameters when querying the \texttt{rtvslo.si} API:

\begin{itemize}
  \item \texttt{q}: the search query string
  \item \texttt{s}: section filter (e.g., \texttt{3} for sports, \texttt{null} for all)
  \item \texttt{sort}: sort order (\texttt{1} = newest first, \texttt{2} = most popular)
  \item \texttt{a}: time range (\texttt{1} = all time, \texttt{2} = last 24 h, \texttt{3} = last week, \texttt{4} = last month, \texttt{5} = last year)
  \item \texttt{per\_page}: number of results to return per page
  \item \texttt{group}: content type group (\texttt{1} = news, \texttt{15} = video, \texttt{16} = audio)
\end{itemize}

\begin{enumerate}
  \item \textbf{Receive user query}:  
    The system accepts a natural‑language question or request from the user.
  \item \textbf{Extract keywords from the query}:  
   Initially we use a spaCy-based named entity recognition (NER) method to identify key entities within the question. If no entities are found, the function assesses the ambiguity of the input using an LLM. If the question is deemed clear enough, keywords are extracted using an LLM-based approach that captures more nuanced information. However, if the text is found to be overly ambiguous, the function generates an explanatory output with a suggestion for a less ambiguous question and terminates further processing. This layered strategy enables efficient handling of clearly defined questions while providing robust feedback for ambiguous queries.
  
  \item \textbf{Perform web‑scraping to discover relevant articles}:  
    Using the extracted keywords, the scraper queries selected news pages to retrieve potentially relevant articles. Since \texttt{robots.txt} is not provided by the site, we enforce a 1s timeout between requests to avoid overwhelming the server.
  
  \item \textbf{Parse and collect article metadata and content}:  
    Relevant content from newly found articles is processed, converted into vector representations (embeddings), and stored in a database. The system retrieves a predefined number of the most semantically similar information chunks from the entire database (including newly added ones) based on the user's query embedding.

    \item \textbf{Chunk Reranking}: The initially retrieved information chunks undergo a refinement step where they are re-evaluated and re-ordered based on their direct relevance to the specific user query, ensuring the most pertinent pieces are prioritized.

    \item \textbf{LLM-based Answer Synthesis}: Finally, a large language model receives the refined, relevant information chunks and the original user query (potentially along with conversation history). It synthesizes this information to generate a comprehensive and contextually appropriate answer.


\end{enumerate}

\subsection{Evaluation}

We evaluate our models using both automatic and human evaluation metrics.  
The automatic metrics include BLEU and ROUGE , while the human evaluation covers coherence, informativeness, and factual consistency.

\paragraph{Automatic Metrics}
Table~\ref{tab:auto-metrics} reports BLEU and ROUGE scores for the two models (GAMS and Gemini) on our test suite.

\begin{table}[h]
  \centering
  \caption{Automatic evaluation results}
  \label{tab:auto-metrics}
  \begin{tabular}{lcccc}
    \toprule
    Model   & BLEU (\%) & ROUGE-1 & ROUGE-2 & ROUGE-L \\
    \midrule
    GAMS    & 30.37     & 0.455   & 0.307   & 0.418   \\
    Gemini  & 57.22     & 0.779   & 0.652   & 0.753   \\
    \bottomrule
  \end{tabular}
\end{table}

\paragraph{Human Evaluation}
For human evaluation, we randomly sampled 11 question--answer pairs and recruited three expert annotators. Each answer was rated on a 1--5 Likert scale for the following dimensions:
\begin{itemize}
  \item \textbf{Coherence}: How well the answer is structured and logically flows.
  \item \textbf{Informativeness}: How much relevant information the answer provides.
  \item \textbf{Factual consistency}: How accurately the answer reflects the ground truth.
\end{itemize}
\clearpage
Notably, GAMS performed the same or worse than Gemini across all human evaluation metrics. Below we highlight question-level findings:
\begin{itemize}
  \item \textbf{Wrong answers (even with context):}
    \begin{itemize}
      \item Q10: Proti kateri ekipi je Benjamin Šeško nazadnje igral? \\
            Model Answer: Nazadnje je Benjamin Šeško igral proti Eintrachtu iz Frankfurta. \\
            Expected: Šeško je nazadnje igral proti Stuttgartu.
      \item Q8: Koliko točk je Dončić dosegel na predzadnji tekmi? \\
            Model Answer: Na predzadnji tekmi je Dončić dosegel 28 točk. \\
            Expected: Dončić je dosegel 38 točk na predzadnji tekmi.
      \item Q5: Koliko obramb je zbral Jan Oblak na zadnji tekmi Atletico Madrida? \\
            Model Answer: Na zadnji tekmi Atletica je Jan Oblak zbral osem obramb. \\
            Expected: Jan Oblak je zbral 6 obramb.
    \end{itemize}
  \item \textbf{Correctly identified absence in context:}
    \begin{itemize}
      \item Q3: Kateri slovenski kolesar je zmagal zadnjo etapo Dirke po Franciji? \\
            Model Answer: Ni v podanem kontekstu.
    \end{itemize}
  \item \textbf{Correct answers:}
    \begin{itemize}
      \item Q1: Koliko točk je Luka Dončić zadel na zadnji tekmi? \\
            Model Answer: Na zadnji tekmi je Luka Dončić dosegel 28 točk.
      \item Q11: Proti kateri ekipi je Anže Kopitar nazadnje igral? \\
            Model Answer: Anže Kopitar je nazadnje igral proti Edmontonu.
    \end{itemize}
\end{itemize}

\paragraph{Discussion}
Our results clearly indicate that Gemini outperforms GAMS across both automatic and human evaluation metrics. The substantial BLEU and ROUGE gains reflect Gemini’s stronger alignment with reference answers, while the higher human scores in coherence, informativeness, and factual consistency demonstrate its superior answer quality and reliability. GAMS’s frequent factual errors—even when context was provided—highlight limitations in its retrieval or reasoning capabilities.




\subsection*{Figures}

You can insert figures that span over the whole page, or over just a single column. The first one, \figurename~\ref{fig:column}, is an example of a figure that spans only across one of the two columns in the report.

\begin{figure}[ht]\centering
	\includegraphics[width=\linewidth]{single_column.pdf}
	\caption{\textbf{A random visualization.} This is an example of a figure that spans only across one of the two columns.}
	\label{fig:column}
\end{figure}

On the other hand, \figurename~\ref{fig:whole} is an example of a figure that spans across the whole page (across both columns) of the report.

% \begin{figure*} makes the figure take up the entire width of the page
\begin{figure*}[ht]\centering 
	\includegraphics[width=\linewidth]{whole_page.pdf}
	\caption{\textbf{Visualization of a Bayesian hierarchical model.} This is an example of a figure that spans the whole width of the report.}
	\label{fig:whole}
\end{figure*}


\subsection*{Tables}

Use the table environment to insert tables.

\begin{table}[hbt]
	\caption{Table of grades.}
	\centering
	\begin{tabular}{l l | r}
		\toprule
		\multicolumn{2}{c}{Name} \\
		\cmidrule(r){1-2}
		First name & Last Name & Grade \\
		\midrule
		John & Doe & $7.5$ \\
		Jane & Doe & $10$ \\
		Mike & Smith & $8$ \\
		\bottomrule
	\end{tabular}
	\label{tab:label}
\end{table}


\subsection*{Code examples}

You can also insert short code examples. You can specify them manually, or insert a whole file with code. Please avoid inserting long code snippets, advisors will have access to your repositories and can take a look at your code there. If necessary, you can use this technique to insert code (or pseudo code) of short algorithms that are crucial for the understanding of the manuscript.

\lstset{language=Python}
\lstset{caption={Insert code directly from a file.}}
\lstset{label={lst:code_file}}
\lstinputlisting[language=Python]{code/example.py}

\lstset{language=R}
\lstset{caption={Write the code you want to insert.}}
\lstset{label={lst:code_direct}}
\begin{lstlisting}
import(dplyr)
import(ggplot)

ggplot(diamonds,
	   aes(x=carat, y=price, color=cut)) +
  geom_point() +
  geom_smooth()
\end{lstlisting}

%------------------------------------------------

\section*{Results}

Use the results section to present the final results of your work. Present the results in a objective and scientific fashion. Use visualisations to convey your results in a clear and efficient manner. When comparing results between various techniques use appropriate statistical methodology.

\subsection*{More random text}

This text is inserted only to make this template look more like a proper report. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Etiam blandit dictum facilisis. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Interdum et malesuada fames ac ante ipsum primis in faucibus. Etiam convallis tellus velit, quis ornare ipsum aliquam id. Maecenas tempus mauris sit amet libero elementum eleifend. Nulla nunc orci, consectetur non consequat ac, consequat non nisl. Aenean vitae dui nec ex fringilla malesuada. Proin elit libero, faucibus eget neque quis, condimentum laoreet urna. Etiam at nunc quis felis pulvinar dignissim. Phasellus turpis turpis, vestibulum eget imperdiet in, molestie eget neque. Curabitur quis ante sed nunc varius dictum non quis nisl. Donec nec lobortis velit. Ut cursus, libero efficitur dictum imperdiet, odio mi fermentum dui, id vulputate metus velit sit amet risus. Nulla vel volutpat elit. Mauris ex erat, pulvinar ac accumsan sit amet, ultrices sit amet turpis.

Phasellus in ligula nunc. Vivamus sem lorem, malesuada sed pretium quis, varius convallis lectus. Quisque in risus nec lectus lobortis gravida non a sem. Quisque et vestibulum sem, vel mollis dolor. Nullam ante ex, scelerisque ac efficitur vel, rhoncus quis lectus. Pellentesque scelerisque efficitur purus in faucibus. Maecenas vestibulum vulputate nisl sed vestibulum. Nullam varius turpis in hendrerit posuere.

Nulla rhoncus tortor eget ipsum commodo lacinia sit amet eu urna. Cras maximus leo mauris, ac congue eros sollicitudin ac. Integer vel erat varius, scelerisque orci eu, tristique purus. Proin id leo quis ante pharetra suscipit et non magna. Morbi in volutpat erat. Vivamus sit amet libero eu lacus pulvinar pharetra sed at felis. Vivamus non nibh a orci viverra rhoncus sit amet ullamcorper sem. Ut nec tempor dui. Aliquam convallis vitae nisi ac volutpat. Nam accumsan, erat eget faucibus commodo, ligula dui cursus nisi, at laoreet odio augue id eros. Curabitur quis tellus eget nunc ornare auctor.


%------------------------------------------------

\section*{Discussion}

Use the Discussion section to objectively evaluate your work, do not just put praise on everything you did, be critical and exposes flaws and weaknesses of your solution. You can also explain what you would do differently if you would be able to start again and what upgrades could be done on the project in the future.


%------------------------------------------------

\section*{Acknowledgments}

Here you can thank other persons (advisors, colleagues ...) that contributed to the successful completion of your project.


%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------
\bibliographystyle{unsrt}
\bibliography{report}


\end{document}